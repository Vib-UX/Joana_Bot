{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Preprocessing for major project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibcYPae1vgKj"
      },
      "source": [
        "import json \r\n",
        "import re\r\n",
        "import numpy as np \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "\r\n",
        "def pad_sequences(sequences,maxlen,padding = 'pre',value=0):\r\n",
        "  padding_sequences = sequences\r\n",
        "  if padding=='pre':\r\n",
        "    for seq in padding_sequences:\r\n",
        "      while(len(seq)<maxlen):\r\n",
        "        seq.insert(0,value)\r\n",
        "  else:\r\n",
        "    for seq in padding_sequences:\r\n",
        "      while(len(seq)<maxlen):\r\n",
        "        seq.append(value)\r\n",
        "  return np.array(padding_sequences)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def texts_to_sequences(training_sentences,word_index):\r\n",
        "  seq=[]\r\n",
        "  bad_char = [',','?','/','_','@','#','*']\r\n",
        "  for sentence in training_sentences:\r\n",
        "    w=[]\r\n",
        "    for c in bad_char:\r\n",
        "      sentence = sentence.replace(c,'')\r\n",
        "    for word in sentence.split():\r\n",
        "      word = word.lower()\r\n",
        "      w.append(word_index[word])\r\n",
        "    seq.append(w)\r\n",
        "  return seq\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class label_encoder:\r\n",
        "\r\n",
        "  def __init__(self,training_labels):\r\n",
        "    self.training_labels = training_labels\r\n",
        "    self.class_array = []\r\n",
        "    self.class_labels = []\r\n",
        "\r\n",
        "\r\n",
        "  def find_class_array(self):\r\n",
        "    self.class_array = []\r\n",
        "    self.class_labels = []\r\n",
        "    idx = 0\r\n",
        "    for data in self.training_labels:\r\n",
        "      if data not in self.class_array:\r\n",
        "        self.class_array.append(data)\r\n",
        "        self.class_labels.append(idx)\r\n",
        "        idx+=1\r\n",
        "    self.class_array = sorted(self.class_array)\r\n",
        "    #print(self.class_array)\r\n",
        "    \r\n",
        "\r\n",
        "  def Label_Encoder(self):\r\n",
        "    num_array = []\r\n",
        "    #print(self.class_array)\r\n",
        "    d = {self.class_array[i]:self.class_labels[i] for i in range(len(self.class_array))}\r\n",
        "    for label in self.training_labels:\r\n",
        "      num_array.append(d[label])\r\n",
        "    return(np.array(num_array))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def inverse_transform(similarity_vector,self):\r\n",
        "    max_index = None\r\n",
        "    max = 0\r\n",
        "    for index in range(len(similarity_vector)):\r\n",
        "      if max<similarity_vector[index]:\r\n",
        "        max_index = index\r\n",
        "        max = similarity_vector[index]\r\n",
        "    return self.class_array[max_index]\r\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn-J-q5NeurV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68761e9d-5fd1-454e-9e8c-50beab3c0681"
      },
      "source": [
        "import nltk\r\n",
        "#nltk.download('stem',quiet=True)\r\n",
        "nltk.download('stopwords', quiet = True)\r\n",
        "nltk.download('wordnet',quiet = True)\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer \r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "stop_words = list(stopwords.words('english'))\r\n",
        "print(stop_words)\r\n",
        "print(lemmatizer.lemmatize(\"how\"))\r\n",
        "if \"how\" in stop_words:\r\n",
        "  print('True')\r\n",
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "how\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHv4tc74fVac"
      },
      "source": [
        "def remove_stopwords(stop_words,training_sentences):\r\n",
        "  new_sentences = []\r\n",
        "  for sentence in training_sentences:\r\n",
        "    s = ''\r\n",
        "    words = sentence.split()\r\n",
        "    for word in words:\r\n",
        "      #word = word.lower()\r\n",
        "      if word not in stop_words:\r\n",
        "        word = lemmatizer.lemmatize(word)\r\n",
        "        s+=(word+' ')\r\n",
        "    s=s.strip()\r\n",
        "    new_sentences.append(s)\r\n",
        "  return new_sentences\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Ef2OAMccqI",
        "outputId": "cec9c924-4625-41f6-b423-ff261881a523"
      },
      "source": [
        "with open('intents.json') as file:\r\n",
        "    data = json.load(file)\r\n",
        "\r\n",
        "\r\n",
        "training_sentences = []\r\n",
        "training_labels = []\r\n",
        "labels = []\r\n",
        "responses = []\r\n",
        "\r\n",
        "\r\n",
        "for intent in data['intents']:\r\n",
        "    for pattern in intent['patterns']:\r\n",
        "        training_sentences.append(pattern)\r\n",
        "        training_labels.append(intent['tag'])\r\n",
        "    responses.append(intent['responses'])\r\n",
        "    \r\n",
        "    if intent['tag'] not in labels:\r\n",
        "        labels.append(intent['tag'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#removing stopwords\r\n",
        "training_sentences = remove_stopwords(stop_words,training_sentences)\r\n",
        "\r\n",
        "document = []\r\n",
        "for i in range(len(training_sentences)):\r\n",
        "  sentence = training_sentences[i]\r\n",
        "  for word in sentence.split():\r\n",
        "    document.append((word,training_labels[i]))\r\n",
        "\r\n",
        "print(document,end='\\n\\n\\n')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(\"before encoding : \",training_labels,end = '\\n\\n')\r\n",
        "\r\n",
        "print(\"after removing stopwords:\")\r\n",
        "print(\"new sentences : \",training_sentences,end = '\\n\\n')\r\n",
        "\r\n",
        "#encoding training labels\r\n",
        "lbl_encoder = label_encoder(training_labels) \r\n",
        "lbl_encoder.find_class_array()\r\n",
        "training_labels = lbl_encoder.Label_Encoder()\r\n",
        "\r\n",
        "\r\n",
        "print(\"after encoding : \",training_labels)\r\n",
        "\r\n",
        "\r\n",
        "vocab_size = 1000\r\n",
        "embedding_dim = 16\r\n",
        "max_len = 20\r\n",
        "oov_token = \"<OOV>\"\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) \r\n",
        "tokenizer.fit_on_texts(training_sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "sequences = texts_to_sequences(training_sentences,word_index)\r\n",
        "padded_sequences = pad_sequences(sequences,max_len)\r\n",
        "\r\n",
        "epochs = 550\r\n",
        "#history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Hi', 'greeting'), ('How', 'greeting'), ('Is', 'greeting'), ('anyone', 'greeting'), ('there?', 'greeting'), ('Hey', 'greeting'), ('Hola', 'greeting'), ('Hello', 'greeting'), ('Good', 'greeting'), ('day', 'greeting'), ('Bye', 'goodbye'), ('See', 'goodbye'), ('later', 'goodbye'), ('Goodbye', 'goodbye'), ('Nice', 'goodbye'), ('chatting', 'goodbye'), ('you,', 'goodbye'), ('bye', 'goodbye'), ('Till', 'goodbye'), ('next', 'goodbye'), ('time', 'goodbye'), ('Thanks', 'thanks'), ('Thank', 'thanks'), (\"That's\", 'thanks'), ('helpful', 'thanks'), ('Awesome,', 'thanks'), ('thanks', 'thanks'), ('Thanks', 'thanks'), ('helping', 'thanks'), ('How', 'options'), ('could', 'options'), ('help', 'options'), ('me?', 'options'), ('What', 'options'), ('do?', 'options'), ('What', 'options'), ('help', 'options'), ('provide?', 'options'), ('How', 'options'), ('helpful?', 'options'), ('What', 'options'), ('support', 'options'), ('offered', 'options'), ('How', 'adverse_drug'), ('check', 'adverse_drug'), ('Adverse', 'adverse_drug'), ('drug', 'adverse_drug'), ('reaction?', 'adverse_drug'), ('Open', 'adverse_drug'), ('adverse', 'adverse_drug'), ('drug', 'adverse_drug'), ('module', 'adverse_drug'), ('Give', 'adverse_drug'), ('list', 'adverse_drug'), ('drug', 'adverse_drug'), ('causing', 'adverse_drug'), ('adverse', 'adverse_drug'), ('behavior', 'adverse_drug'), ('List', 'adverse_drug'), ('drug', 'adverse_drug'), ('suitable', 'adverse_drug'), ('patient', 'adverse_drug'), ('adverse', 'adverse_drug'), ('reaction', 'adverse_drug'), ('Which', 'adverse_drug'), ('drug', 'adverse_drug'), ('dont', 'adverse_drug'), ('adverse', 'adverse_drug'), ('reaction?', 'adverse_drug'), ('Open', 'blood_pressure'), ('blood', 'blood_pressure'), ('pressure', 'blood_pressure'), ('module', 'blood_pressure'), ('Task', 'blood_pressure'), ('related', 'blood_pressure'), ('blood', 'blood_pressure'), ('pressure', 'blood_pressure'), ('Blood', 'blood_pressure'), ('pressure', 'blood_pressure'), ('data', 'blood_pressure'), ('entry', 'blood_pressure'), ('I', 'blood_pressure'), ('want', 'blood_pressure'), ('log', 'blood_pressure'), ('blood', 'blood_pressure'), ('pressure', 'blood_pressure'), ('result', 'blood_pressure'), ('Blood', 'blood_pressure'), ('pressure', 'blood_pressure'), ('data', 'blood_pressure'), ('management', 'blood_pressure'), ('I', 'blood_pressure_search'), ('want', 'blood_pressure_search'), ('search', 'blood_pressure_search'), ('blood', 'blood_pressure_search'), ('pressure', 'blood_pressure_search'), ('result', 'blood_pressure_search'), ('history', 'blood_pressure_search'), ('Blood', 'blood_pressure_search'), ('pressure', 'blood_pressure_search'), ('patient', 'blood_pressure_search'), ('Load', 'blood_pressure_search'), ('patient', 'blood_pressure_search'), ('blood', 'blood_pressure_search'), ('pressure', 'blood_pressure_search'), ('result', 'blood_pressure_search'), ('Show', 'blood_pressure_search'), ('blood', 'blood_pressure_search'), ('pressure', 'blood_pressure_search'), ('result', 'blood_pressure_search'), ('patient', 'blood_pressure_search'), ('Find', 'blood_pressure_search'), ('blood', 'blood_pressure_search'), ('pressure', 'blood_pressure_search'), ('result', 'blood_pressure_search'), ('ID', 'blood_pressure_search'), ('Find', 'pharmacy_search'), ('pharmacy', 'pharmacy_search'), ('Find', 'pharmacy_search'), ('pharmacy', 'pharmacy_search'), ('List', 'pharmacy_search'), ('pharmacy', 'pharmacy_search'), ('nearby', 'pharmacy_search'), ('Locate', 'pharmacy_search'), ('pharmacy', 'pharmacy_search'), ('Search', 'pharmacy_search'), ('pharmacy', 'pharmacy_search'), ('Lookup', 'hospital_search'), ('hospital', 'hospital_search'), ('Searching', 'hospital_search'), ('hospital', 'hospital_search'), ('transfer', 'hospital_search'), ('patient', 'hospital_search'), ('I', 'hospital_search'), ('want', 'hospital_search'), ('search', 'hospital_search'), ('hospital', 'hospital_search'), ('data', 'hospital_search'), ('Hospital', 'hospital_search'), ('lookup', 'hospital_search'), ('patient', 'hospital_search'), ('Looking', 'hospital_search'), ('hospital', 'hospital_search'), ('detail', 'hospital_search')]\n",
            "\n",
            "\n",
            "before encoding :  ['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'options', 'options', 'options', 'options', 'options', 'adverse_drug', 'adverse_drug', 'adverse_drug', 'adverse_drug', 'adverse_drug', 'blood_pressure', 'blood_pressure', 'blood_pressure', 'blood_pressure', 'blood_pressure', 'blood_pressure_search', 'blood_pressure_search', 'blood_pressure_search', 'blood_pressure_search', 'blood_pressure_search', 'pharmacy_search', 'pharmacy_search', 'pharmacy_search', 'pharmacy_search', 'pharmacy_search', 'hospital_search', 'hospital_search', 'hospital_search', 'hospital_search', 'hospital_search']\n",
            "\n",
            "after removing stopwords:\n",
            "new sentences :  ['Hi', 'How', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day', 'Bye', 'See later', 'Goodbye', 'Nice chatting you, bye', 'Till next time', 'Thanks', 'Thank', \"That's helpful\", 'Awesome, thanks', 'Thanks helping', 'How could help me?', 'What do?', 'What help provide?', 'How helpful?', 'What support offered', 'How check Adverse drug reaction?', 'Open adverse drug module', 'Give list drug causing adverse behavior', 'List drug suitable patient adverse reaction', 'Which drug dont adverse reaction?', 'Open blood pressure module', 'Task related blood pressure', 'Blood pressure data entry', 'I want log blood pressure result', 'Blood pressure data management', 'I want search blood pressure result history', 'Blood pressure patient', 'Load patient blood pressure result', 'Show blood pressure result patient', 'Find blood pressure result ID', 'Find pharmacy', 'Find pharmacy', 'List pharmacy nearby', 'Locate pharmacy', 'Search pharmacy', 'Lookup hospital', 'Searching hospital transfer patient', 'I want search hospital data', 'Hospital lookup patient', 'Looking hospital detail']\n",
            "\n",
            "after encoding :  [4 4 4 4 4 4 4 3 3 3 3 3 8 8 8 8 8 6 6 6 6 6 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2\n",
            " 7 7 7 7 7 5 5 5 5 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bab1GOA8-fah",
        "outputId": "dc4a29d9-8718-45da-83be-856bae7c1b3b"
      },
      "source": [
        "seq = texts_to_sequences(training_sentences,word_index)\r\n",
        "print(seq)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[26], [10], [27, 28, 29], [30], [31], [32], [33, 34], [20], [35, 36], [37], [38, 39, 40, 20], [41, 42, 43], [11], [44], [45, 21], [46, 11], [11, 47], [10, 48, 22, 49], [12, 50], [12, 22, 51], [10, 21], [12, 52, 53], [10, 54, 5, 6, 13], [23, 5, 6, 24], [55, 14, 6, 56, 5, 57], [14, 6, 58, 4, 5, 13], [59, 6, 60, 5, 13], [23, 2, 3, 24], [61, 62, 2, 3], [2, 3, 15, 63], [16, 17, 64, 2, 3, 7], [2, 3, 15, 65], [16, 17, 18, 2, 3, 7, 66], [2, 3, 4], [67, 4, 2, 3, 7], [68, 2, 3, 7, 4], [19, 2, 3, 7, 69], [19, 8], [19, 8], [14, 8, 70], [71, 8], [18, 8], [25, 9], [72, 9, 73, 4], [16, 17, 18, 9, 15], [9, 25, 4], [74, 9, 75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDARORm9G_Wr",
        "outputId": "b980ccb1-cdb7-4d19-fb7c-e5374b9e172f"
      },
      "source": [
        "print(padded_sequences)\r\n",
        "#padded_sequences2[0,:].shape\r\n",
        "# if seq == sequences:\r\n",
        "#   print('true')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 27 28 29]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 31]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 33 34]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 35 36]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 37]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 38 39 40 20]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 41 42 43]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 44]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 45 21]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11 47]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 48 22 49]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 50]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 22 51]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 21]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 52 53]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 54  5  6 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  5  6 24]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 55 14  6 56  5 57]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  6 58  4  5 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 59  6 60  5 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  2  3 24]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 61 62  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3 15 63]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 16 17 64  2  3  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3 15 65]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 16 17 18  2  3  7 66]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 67  4  2  3  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 68  2  3  7  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  2  3  7 69]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  8 70]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 71  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 72  9 73  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16 17 18  9 15]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9 25  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  9 75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_0WCIRSZ7z7"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEKNik1r9n6a"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMW5ZNXU9-kX"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWC20srd-3_T"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol3pdZ9D_KVf"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8p4cPJYCgQQ"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}
